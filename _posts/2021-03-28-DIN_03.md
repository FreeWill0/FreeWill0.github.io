---
layout:     post
title:      DIN
subtitle:   03
date:       2021-3-28
author:     JieKun Liu
header-img: img/the-first.png
catalog:   true
tags:
    - Recommender System
---
## 概述

所设计的DIN模型在计算用户兴趣向量时，能够自适应地考虑用户的历史行为与候选广告之间的关系。

对于不同的候选广告，这种用户兴趣向量是不同的。

至于如何在网络结构中实现，DIN引入了一个本地激活单元来计算用户行为特征与候选广告之间的关系
## 代码 & 心得
    import tensorflow as tf
    from Dice import dice
    class Model(object):
      def __init__(self, user_count, item_count, cate_count, cate_list,\
                                   predict_batch_size, predict_ads_num):
        self.u = tf.placeholder(tf.int32, [None,]) 
        # shape: [B],  user id。 (B：batch size)
        self.i = tf.placeholder(tf.int32, [None,]) 
        # shape: [B]  i: 正样本的item
        self.j = tf.placeholder(tf.int32, [None,]) 
        # shape: [B]  j: 负样本的item
        self.y = tf.placeholder(tf.float32, [None,]) 
        # shape: [B], y: label
        self.hist_i = tf.placeholder(tf.int32, [None, None]) 
        # shape: [B, T] #用户行为特征(User Behavior)中的item序列。T为序列长度
        self.sl = tf.placeholder(tf.int32, [None,]) 
        # shape: [B]; sl：sequence length，User Behavior中序列的真实序列长度（？）
        self.lr = tf.placeholder(tf.float64, [])
        # learning rate
        hidden_units = 128
        user_emb_w = tf.get_variable("user_emb_w", [user_count, hidden_units])       
        # shape: [U, H], user_id的embedding weight. U是user_id的hash bucket size

        item_emb_w = tf.get_variable("item_emb_w", [item_count, hidden_units // 2])  #[I, H//2]
         # shape: [I, H//2], item_id的embedding weight. I是item_id的hash bucket size

        item_b = tf.get_variable("item_b", [item_count],
                                 initializer=tf.constant_initializer(0.0))           
        # shape: [I], bias
        cate_emb_w = tf.get_variable("cate_emb_w", [cate_count, hidden_units // 2])  
        # shape: [C, H//2], cate_id的embedding weight. 

        cate_list = tf.convert_to_tensor(cate_list, dtype=tf.int64)   
        # shape: [C, H//2]

        ic = tf.gather(cate_list, self.i) 
        # 从cate_list中取出正样本的cate
        i_emb = tf.concat(values = [   
            tf.nn.embedding_lookup(item_emb_w, self.i),
            tf.nn.embedding_lookup(cate_emb_w, ic),
            ], axis=1)
        # 正样本的embedding，正样本包括item和cate

        i_b = tf.gather(item_b, self.i)

        jc = tf.gather(cate_list, self.j) 
        # 从cate_list中取出负样本的cate
        j_emb = tf.concat([             
            tf.nn.embedding_lookup(item_emb_w, self.j),
            tf.nn.embedding_lookup(cate_emb_w, jc),
            ], axis=1)
        # 负样本的embedding，负样本包括item和cate

        j_b = tf.gather(item_b, self.j) #偏置b
        hc = tf.gather(cate_list, self.hist_i) 
        # 用户行为序列(User Behavior)中的cate序列

        h_emb = tf.concat([tf.nn.embedding_lookup(item_emb_w, self.hist_i),
            tf.nn.embedding_lookup(cate_emb_w, hc),
            ], axis=2)
        #用户行为序列(User Behavior)的embedding，包括item序列和cate序列
        hist_i = attention(i_emb, h_emb, self.sl) #attention操作
        #-- attention end ---
